[
  {
    "path": "posts/2021-05-18-nearest-neighbors-classifier/",
    "title": "Nearest Neighbors Classifier",
    "description": "Birds of same feather flock together",
    "author": [
      {
        "name": "Vidyasagar Bhargava",
        "url": {}
      }
    ],
    "date": "2021-05-18",
    "categories": [
      "Machine Learning",
      "Python"
    ],
    "contents": "\n\nContents\nIntroduction\nk-NN algorithm\nChoosing an appropriate k\nk-NN from scratch\nk-NN Classifier\nk-NN regressor\n\n\nIntroduction\nNearest neighbor classifiers are defined by their characteristic of classifying unlabeled examples by assigning them the class of similar labeled examples.\nNearest Neighbors works well for classification task where the relationships among the features and target classes are numerous and extremely difficult to understand but the items of similar class tend to be homogeneous.\nNearest Neighbor classifier struggles most when there is no clear distinction exists among the groups.\nk-NN algorithm\nk-Nearest Neighbor algorithm is an example of nearest neighbor classifier.\n     Strength\n        Weakness\nSimple and effective\nMakes no assumption about data\nFast Training Process\n\nDoesn’t produce model, limiting the ability to understand how features are related to class\nRequires selection of k\nSlow classification phase\nCategorical features and missing data require pre processing\n\nThe k-Nearest Neighbor algorithm uses nearest k number of neighbors for labeling of an unlabeled example. The unlabeled test example is assigned the class of majority of the k-Nearest Neighbors.\nFor finding the distance k-NN algorithm uses Euclidean distance.\nChoosing an appropriate k\nChoosing the value of k determines how well the model will generalize to future data. Choosing a large k reduces the impact or variance caused by noisy data but can bias the learner so that it runs the risk of ignoring small, but important pattern.\nIn Practice choosing k depends on the difficulty of the concept to be learned and the number of records in training data.\nStart with k value equal to the square root of the number of training examples.\nUsing cross validation to determine the best k value.\nWeighted voting is one of interesting way to solve this problem. By giving higher weight to close neighbors.b 444\nk-NN from scratch\nCompute distances between x and all examples in the training set\nSort by distance and return indexes of the first k neighbors\nExtract the labels of the k nearest neighbor training samples\nReturn the most common class label\n\nimport numpy as np\nfrom collections import Counter\n\n\ndef euclidean_distance(x1, x2):\n        return np.sqrt(np.sum((x1 - x2)**2))\n\nWe used euclidean distance for calculating the nearest neighbors.\nNow we will define our KNN Class\n\nclass KNN:\n    def __init__(self, k=3):\n      self.k = k\n\n    def fit(self, X, y):\n      self.X_train = X\n      self.y_train = y\n\n    def predict(self, X):\n      y_pred = [self._predict(x) for x in X]\n      return np.array(y_pred)\n\n    def _predict(self, x):\n        # Compute distances between x and all examples in the training set\n      distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n        # Sort by distance and return indexes of the first k neighbors\n      k_idx = np.argsort(distances)[:self.k]\n        # Extract the labels of the k nearest neighbor training samples\n      k_neighbor_labels = [self.y_train[i] for i in k_idx]  \n        # return the most common class label\n      most_common = Counter(k_neighbor_labels).most_common(1)\n      return most_common[0][0]\n\nWe are going to use iris dataset to test our KNN model that we created !!!\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\niris = datasets.load_iris()\n\nX, y = iris.data, iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n\n\nclf = KNN()\nclf.fit(X_train, y_train)\n\n\ndef accuracy(y_true, y_pred):\n    accuracy = np.sum(y_true == y_pred) / len(y_true)\n    return accuracy\n\n\npredictions = clf.predict(X_test)\nprint(\"custom KNN classification accuracy\", accuracy(y_test, predictions))\ncustom KNN classification accuracy 1.0\n\nk-NN Classifier\nDefining the dataset for k-NN\n\n# First Feature\nweather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',\n'Rainy','Sunny','Overcast','Overcast','Rainy']\n# Second Feature\ntemp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']\n\n# Label or target varible\nplay=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']\n\nWe have two features weather and temperature and one label play.\nEncoding data columns\n\nfrom sklearn import preprocessing\n\n#creating labelEncoder\nle = preprocessing.LabelEncoder()\n\n# Converting string labels into numbers.\nweather_encoded=le.fit_transform(weather)\nprint(weather_encoded)\n[2 2 0 1 1 1 0 2 2 1 2 0 0 1]\n\nSimilarly, you can encode temperature and label into numeric columns.\n\n# converting string labels into numbers\ntemp_encoded=le.fit_transform(temp)\nlabel=le.fit_transform(play)\n\nCombining Features\n\n#combinig weather and temp into single listof tuples\nfeatures=list(zip(weather_encoded,temp_encoded))\n\nGenerating Model\nLet’s build k-NN classifier model\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=3)\n\n# Train the model using the training sets\nmodel.fit(features,label)\n\n#Predict Output\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n                     weights='uniform')\npredicted= model.predict([[0,2]]) # 0:Overcast, 2:Mild\nprint(predicted)\n[1]\n\nk-NN regressor\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.neighbors import KNeighborsRegressor\nX,y = load_boston(return_X_y=True)\nmod = KNeighborsRegressor()\nmod.fit(X,y)\nKNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n                    metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n                    weights='uniform')\nmod.predict(X)\narray([21.78, 22.9 , 25.36, 26.06, 27.1 , 27.1 , 20.88, 19.1 , 18.4 ,\n       19.48, 19.28, 22.  , 24.34, 20.52, 24.66, 21.3 , 30.48, 20.4 ,\n       15.7 , 23.54, 16.82, 17.64, 18.3 , 17.08, 16.66, 15.1 , 16.78,\n       14.94, 19.94, 18.34, 14.1 , 16.82, 15.12, 14.1 , 15.12, 26.92,\n       22.14, 27.4 , 28.44, 31.88, 31.88, 25.36, 25.36, 24.22, 20.68,\n       20.44, 20.44, 18.1 , 18.1 , 24.  , 21.54, 24.  , 27.16, 27.16,\n       25.7 , 39.82, 27.08, 38.28, 24.8 , 25.64, 21.78, 33.6 , 21.78,\n       24.06, 31.74, 25.3 , 26.98, 22.18, 20.42, 20.42, 27.76, 29.5 ,\n       27.76, 27.76, 22.92, 21.64, 25.82, 21.64, 21.38, 22.02, 24.8 ,\n       21.88, 25.22, 25.64, 25.98, 25.98, 23.28, 25.98, 24.02, 25.58,\n       25.58, 25.06, 26.34, 26.04, 30.1 , 24.84, 23.62, 24.32, 28.52,\n       24.96, 22.1 , 22.2 , 15.34, 19.74, 19.74, 19.66, 19.56, 21.34,\n       19.66, 19.56, 22.08, 20.1 , 19.6 , 17.54, 20.1 , 17.7 , 20.2 ,\n       20.1 , 20.66, 19.8 , 22.76, 20.6 , 19.66, 18.52, 19.66, 20.6 ,\n       18.52, 16.62, 18.04, 16.88, 18.4 , 18.4 , 18.78, 18.56, 20.24,\n       17.44, 17.8 , 18.4 , 15.88, 17.06, 15.24, 14.76, 15.62, 15.62,\n       15.62, 18.26, 18.26, 15.62, 17.82, 17.44, 37.22, 20.66, 19.28,\n       20.24, 20.24, 15.34, 15.34, 37.78, 25.52, 32.08, 20.66, 42.56,\n       44.54, 44.54, 30.34, 20.24, 37.22, 19.52, 19.98, 20.24, 19.98,\n       18.74, 21.9 , 24.4 , 23.74, 25.82, 22.34, 24.2 , 23.84, 38.56,\n       33.24, 38.56, 33.24, 31.6 , 33.24, 38.56, 37.84, 32.72, 33.14,\n       32.72, 33.14, 32.72, 33.72, 31.8 , 31.8 , 34.9 , 26.78, 25.24,\n       26.78, 29.38, 29.5 , 25.2 , 25.3 , 41.28, 41.28, 23.76, 23.78,\n       20.74, 22.9 , 21.1 , 21.1 , 21.1 , 23.9 , 28.84, 22.6 , 27.28,\n       22.96, 21.9 , 21.1 , 23.38, 25.42, 17.32, 31.8 , 24.46, 37.82,\n       36.46, 36.14, 35.96, 29.5 , 29.44, 34.  , 41.28, 38.28, 38.16,\n       28.12, 29.3 , 34.12, 34.12, 21.44, 21.92, 21.44, 21.4 , 22.1 ,\n       21.74, 20.  , 19.68, 22.16, 20.  , 21.38, 31.22, 31.22, 26.28,\n       29.56, 31.22, 27.08, 24.86, 38.28, 42.44, 38.9 , 36.48, 38.82,\n       41.88, 41.88, 37.9 , 41.88, 34.6 , 38.82, 36.16, 32.42, 31.74,\n       32.58, 28.82, 31.74, 26.88, 31.96, 31.96, 31.96, 31.96, 31.96,\n       30.58, 31.74, 32.58, 36.62, 42.8 , 24.84, 21.88, 38.64, 21.88,\n       24.44, 22.62, 34.9 , 34.9 , 31.88, 24.54, 23.28, 24.44, 23.22,\n       22.94, 25.28, 29.12, 25.42, 25.78, 28.02, 30.58, 31.22, 27.02,\n       31.96, 27.02, 23.72, 21.6 , 29.  , 21.32, 21.02, 20.94, 21.44,\n       21.6 , 18.54, 19.52, 20.5 , 21.3 , 23.32, 23.76, 23.32, 22.9 ,\n       22.06, 23.76, 26.14, 22.06, 19.7 , 21.22, 19.92, 21.86, 22.98,\n       23.6 , 21.16, 20.78, 25.74, 24.3 , 20.72, 22.64, 24.32, 24.44,\n       19.8 , 29.36, 26.58, 19.  , 18.84, 26.48, 33.32, 25.78, 25.78,\n       28.  , 30.46, 42.8 , 20.96, 24.8 , 15.88, 19.06, 20.94, 21.42,\n       33.8 , 25.6 , 30.94, 25.6 , 27.22, 27.22, 16.98, 14.58, 39.16,\n       39.16, 26.46, 36.6 , 27.22, 11.36, 10.54, 10.82, 12.36, 12.5 ,\n        9.6 , 10.22,  6.86, 10.26, 11.62, 11.62, 14.16, 11.  ,  8.94,\n        9.74, 13.18, 12.5 , 13.52, 21.66, 11.88, 15.58, 11.74, 13.54,\n       13.98, 12.46,  8.5 , 14.82,  9.54, 11.14, 15.88, 10.5 , 14.28,\n        6.86, 13.18, 18.56, 14.52, 17.62, 10.34, 12.74, 11.88, 17.18,\n       10.92, 11.88, 11.2 , 14.58, 11.96, 10.44, 16.98, 16.98, 14.16,\n       13.58, 12.94, 11.74, 12.56, 10.26, 13.52, 11.58, 14.  , 12.6 ,\n       13.52, 13.3 , 12.9 , 12.16, 12.1 , 10.56, 12.12, 11.96, 10.6 ,\n       14.56, 13.84, 13.34, 13.44, 12.44, 16.98, 13.34, 14.48, 16.06,\n       13.58, 15.54, 17.04, 16.62, 12.54, 12.2 , 13.58, 12.94, 14.84,\n       20.24, 14.34, 19.82, 20.24, 20.38, 22.28, 17.56, 12.74, 18.56,\n       23.7 , 21.26, 20.24, 18.6 , 22.72, 23.28, 15.54, 16.06, 14.18,\n       14.96, 15.88, 16.36, 22.28, 22.72, 23.44, 20.86, 22.8 , 21.34,\n       21.42, 21.34, 17.04, 11.54, 12.28, 14.86, 18.3 , 22.08, 21.82,\n       22.02, 18.7 , 18.7 , 20.64, 18.7 , 19.96, 21.18, 23.12, 20.88,\n       21.9 , 21.42])\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-20T16:22:36+05:30",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-17-lets-talk-about-probability-distribution/",
    "title": "Let's Talk about Probability Distribution",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Vidyasagar Bhargava",
        "url": "https://vidyasagar.rbind.io"
      }
    ],
    "date": "2021-03-17",
    "categories": [],
    "contents": "\nOk so you have already heard of term probability Distribution but it always confuses you then you are at right place.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndata_url = 'https://raw.githubusercontent.com/chiphuyen/just-pandas-things/master/data/interviews.csv'\ndf = pd.read_csv(data_url)\ndf.head()\n  Company  ...                                             Review\n0   Apple  ...  Application  I applied through a staffing agen...\n1   Apple  ...  Application  I applied online. The process too...\n2   Apple  ...  Application  The process took 4 weeks. I inter...\n3   Apple  ...  Application  The process took a week. I interv...\n4   Apple  ...  Application  I applied through an employee ref...\n\n[5 rows x 10 columns]\n\n\ndf.groupby(['Company']).size().reset_index(name='interviews').sort_values(['interviews'], ascending=False).head(10)\n      Company  interviews\n2      Amazon        3469\n7      Google        3445\n6    Facebook        1817\n11  Microsoft        1790\n8         IBM         873\n4       Cisco         787\n14     Oracle         701\n25       Uber         445\n26       Yelp         404\n3       Apple         363\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-25T22:40:47+05:30",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-15-fun-with-linear-regression/",
    "title": "Fun with Linear Regression",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Vidyasagar Bhargava",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-03-15",
    "categories": [],
    "contents": "\nLinear Regression :-\n1. How it works and Building from scratch?\n2. What are assumptions?\n3. How to interpret model output?\n4. How to evaluate model performance?\n5. Other topics\nHow it works?\nimporting key libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimporting dataset\ndf = pd.read_csv(\"https://raw.githubusercontent.com/mohit-baliyan/References/master/salary_data.csv\")\ndf.head()\n   YearsExperience  Salary\n0              1.1   39343\n1              1.3   46205\n2              1.5   37731\n3              2.0   43525\n4              2.2   39891\n\n\nX = df.iloc[:,0]\nY = df.iloc[:,1]\nplt.scatter(X, Y)\nplt.show()\n\n\nbuilding class\n\nclass LinearHypothesis():\n    def __init__(self):\n        self.w = np.random.randn()\n        self.b = np.random.randn()\n    \n    def __call__(self, X):\n        pred = self.w*X + self.b\n        return pred\n        \n    def update_params(self,new_w, new_b):\n        self.new_w = w\n        self.new_b = b\n\nOutput\n\nH = LinearHypothesis()\ny_hat = H(X)\nprint('Input :', X, '\\n')\nInput : 0      1.1\n1      1.3\n2      1.5\n3      2.0\n4      2.2\n5      2.9\n6      3.0\n7      3.2\n8      3.2\n9      3.7\n10     3.9\n11     4.0\n12     4.0\n13     4.1\n14     4.5\n15     4.9\n16     5.1\n17     5.3\n18     5.9\n19     6.0\n20     6.8\n21     7.1\n22     7.9\n23     8.2\n24     8.7\n25     9.0\n26     9.5\n27     9.6\n28    10.3\n29    10.5\nName: YearsExperience, dtype: float64 \nprint('Weight :', H.w, 'B :', H.b, '\\n')\nWeight : 0.5662352546949777 B : -1.4489134978747804 \nprint('prediction :', y_hat, '\\n')\nprediction : 0    -0.826055\n1    -0.712808\n2    -0.599561\n3    -0.316443\n4    -0.203196\n5     0.193169\n6     0.249792\n7     0.363039\n8     0.363039\n9     0.646157\n10    0.759404\n11    0.816028\n12    0.816028\n13    0.872651\n14    1.099145\n15    1.325639\n16    1.438886\n17    1.552133\n18    1.891875\n19    1.948498\n20    2.401486\n21    2.571357\n22    3.024345\n23    3.194216\n24    3.477333\n25    3.647204\n26    3.930321\n27    3.986945\n28    4.383310\n29    4.496557\nName: YearsExperience, dtype: float64 \n\nGradient descent\n\n\n\n",
    "preview": "posts/2021-03-15-fun-with-linear-regression/fun-with-linear-regression_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-15T15:04:30+05:30",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
